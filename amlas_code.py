# -*- coding: utf-8 -*-
"""amlas_code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-Ax6TrJr-waJ8dL_l6tBdlAs8zMgV6gb
"""

import multiprocessing
import numpy as np
import pandas as pd
from collections import Counter
from matplotlib import pyplot as plt
from itertools import accumulate
import random

# Set the random seed to 42
random.seed(11)

#read in the ratings
ratings = pd.read_csv("ratings.csv") #sorted by users
movies =  pd.read_csv("moviess.csv") #movies and their ids
ratings_mov = ratings[['movieId', 'userId', 'rating']].sort_values('movieId').reset_index(drop=True)  #sorted by movies

#function used to obtain start indexes map
def get_unique_start_indexes(lst):
    unique_vals = set()
    start_indexes = {}
    for i, val in enumerate(lst):
        if val not in unique_vals:
            unique_vals.add(val)
            start_indexes[val] = i
    return start_indexes

#function to obtain internal ids
def internal_id(start_map, og_id):
    values = list(start_map.values())
    lst = []
    for i in range(0,len(values)):
        start = values[i]
        if i == len(values) - 1:
            end = len(og_id)
        else: end = values[i+1] 
        lst.extend(list(range(end-start)))
        
    return lst

#start indexes
movie_Id = list(range(0, len(np.unique(ratings_mov['movieId'].values)))) #continous movie ids
movie_map = dict(zip(np.unique(ratings_mov['movieId'].values), movie_Id)) #map continous movie Id to original Ids
start_ind_usr = get_unique_start_indexes(list(ratings['userId']))        #start indexes of users
start_ind_mov = get_unique_start_indexes(list(ratings_mov['movieId']))  #start indexes of movies

movie_Id_og = ratings["movieId"].tolist() #original movie Ids
int_movieId = internal_id(start_ind_usr, movie_Id_og) #internal movie ids for (a) users  indexes

#(userId, internalMovieId, MovieId )map internal movie ids to system movie id  for each user
map_movieId = list(zip(ratings["userId"].tolist(),int_movieId,movie_Id_og))

user_Id = ratings_mov["userId"].tolist() #original user Ids
int_userId = internal_id(start_ind_mov, user_Id) #internal user ids for (b) movies indexes
        
#(movieId, internalUserId, UserId)map internal user ids to system user id for each movie        
map_userId = list(zip(ratings_mov["movieId"].tolist(),int_userId,user_Id))

# data structures - for indexing by userId
rlist = ratings.iloc[:,2].tolist()   # ratings  by different users                                               
Index_user = list(zip(int_movieId, rlist))

# data structures - for indexing by movieId
rlist_m = ratings_mov.iloc[:,2].tolist()  #ratings given to each movie                                                   
Index_movie = list(zip(int_userId, rlist_m))

# distribution of ratings
ratings_dat = ratings["rating"]
fig, axs= plt.subplots(figsize =(10, 7))
axs.hist(ratings_dat)
plt.xlabel("ratings")
plt.ylabel("count")
plt.title("Histogram of ratings")
plt.show()

n_users = ratings.groupby("userId")["movieId"].count() # degree distribution for users
n_movie = ratings.groupby("movieId")["userId"].count() # degree distribution for movies

users_deg = list(n_users.to_dict().values())
freq_users = Counter(users_deg)                  #frequency of degree for users
movie_deg = list(n_movie.to_dict().values())
freq_movie = Counter(movie_deg)                  #frequency of degrees for movies

list_u = sorted(freq_users.items()) # order by degree for users
xu, yu = zip(*list_u)
list_m = sorted(freq_movie.items()) # order by degree for movies
xm, ym = zip(*list_m)

#plot
plt.yscale("log")
plt.xscale("log")
plt.xlabel("degrees")
plt.ylabel("frequency")
plt.scatter(xu,yu, color='blue')
plt.scatter(xm,ym, color='green')
plt.legend(["users","movies"])
plt.show()

#ML estimate for user + item biases using alternating least squares
m_usrs = np.unique(ratings["userId"])  # list of userId
n_movies = np.unique(ratings_mov["movieId"])  #list of movieId
lamb_value =  0.1 #lambda value
tau = 0.01 #tau value
alpha = 0.001 #alpha values

# initialize biases
b_m = np.array([0]*len(m_usrs)) #bias terms of users
b_n = np.array([0]*len(n_movies))#bias terms of movies

# initialize bias terms for bias updates only
b_usrs = np.array([0]*len(m_usrs)) #bias terms of users
b_movies = np.array([0]*len(n_movies))#bias terms of movies

# initialize user and movie vectors
dimension = 20
mean = 0
std_dev = 2.5 / np.sqrt(10)
U = np.random.normal(loc=mean, scale=std_dev, size=(len(m_usrs), dimension)) #users
V = np.random.normal(loc=mean, scale=std_dev, size=(len(n_movies), dimension)) #items

# functions for user bias updates only
def user_update_bias(m):
    global start_ind_usr, Index_user, map_movieId, b_usrs, b_movies, n_movies, movie_map
    
    rate_vec = np.array([t[1] for t in [Index_user[i] for i in start_end_u(m)]]) #ratings given by mth user
    movies =  list([t[-1] for t in [map_movieId[k] for k in start_end_u(m)]]) # movies rated by mth user
    movie_index  = list([movie_map[j] for j in movies])   #continous indexes of those movies
    b_mov = b_movies[movie_index] #bias terms of movies rated by mth user
    b_usrs[m-1] = lamb_value*(np.sum(rate_vec - b_mov))/(lamb_value*len(movies) + alpha) #bias term update

#function for updating movie bias terms
def movies_update_bias(n):
    global start_ind_mov, Index_movie, map_userId, b_usrs, b_movies, m_usrs, movie_map
    
    j = movie_map[n]  #index of nth movie
    rate_vec = np.array([t[1] for t in [Index_movie[i] for i in start_end_m(j)]]) #ratings given to nth movie
    users = list([t[-1] for t in [map_userId[k] for k in start_end_m(j)]]) #users who rated jth movie
    b_users = b_usrs[[k-1 for k in users]] #bias terms of users who rated nth movie
    b_movies[j] = lamb_value*(np.sum(rate_vec - b_users))/(lamb_value*len(users) + alpha) #bias term update
    
 #function for obtaining cost function
def cost_update_bias(q):
    global ratings_mov, Index_movie, start_ind_mov, n_movies, b_usrs, b_movies
    
    cost_bias = 0
    j = movie_map[q] #index of qth movie
    rate_vec = np.array([t[1] for t in [Index_movie[l] for l in start_end_m(j)]]) #ratings array
    users = list([t[-1] for t in [map_userId[m] for m in start_end_m(j)]]) #users who rated jth movie
    b_users = b_usrs[[x - 1 for x in users]] #bias terms of users who watched qth movie
    b_mov = np.full(len(users), b_movies[j])  #bias of qth movie
    cost_bias = np.sum(np.square(rate_vec - b_users - b_mov))
    return cost_bias  

def start_end_m(q):
    global start_ind_mov; global Index_movie
    
    values = list(start_ind_mov.values())
    start = values[q]
    if q == len(values) - 1:
        end = len(Index_movie)
    else:
        end = values[q+1]
    return range(start,end)

def start_end_u(m):
    global start_ind_usr; global Index_user
    
    values = list(start_ind_usr.values())
    start = values[m-1]
    if (m-1) == len(values) - 1:
        end = len(Index_user)
    else:
        end = values[m]
    return range(start,end)

### update the bias terms only
epoch = 0
cost_bias = []
rmse_bias = []

while epoch < 20:
    #updating user biases
    for m in m_usrs:
        user_update_bias(m) 
        
    #updating movie biases
    for n in n_movies:
        movies_update_bias(n)
    
    #obtaining the cost function and rmse
    sum_cost_bias = 0
    for q in n_movies:
        sum_cost_bias += cost_update_bias(q)
        
            
    cost_bias.append(-(lamb_value/2)*sum_cost_bias - (alpha/2)*np.sum(np.square(b_usrs)) - (alpha/2)*np.sum(np.square(b_movies)))  
    rmse_bias.append(np.sqrt((1/len(n_movies))*sum_cost_bias))
    
    epoch += 1
    
    print("End of iteration: ", epoch)
    
# Cost function plot
m = list(range(len(cost_bias)))
plt.plot(m, cost_bias)
plt.xlabel("iteration")
plt.ylabel("Log Likelihood")
plt.title("Plot of Log Likelihood function")
plt.show()

# RMSE plot
l = list(range(len(rmse_bias)))
plt.plot(l, rmse_bias)
plt.xlabel("iteration")
plt.ylabel("rmse")
plt.title("Plot of RMSE")
plt.show()

#function for updating user bias terms + trait vectors
def user_update(m):
    global start_ind_usr; global Index_user; global map_movieId
    global U; global V; global b_n; global b_m; global n_movies; global movie_map
    
    rate_vec = np.array([t[1] for t in [Index_user[i] for i in start_end_u(m)]]) #ratings given by mth user
    u_user = U[m-1] #trait vector of mth user
    movies =  list([t[-1] for t in [map_movieId[k] for k in start_end_u(m)]]) # movies rated by mth user
    movie_index  = list([movie_map[j] for j in movies])   #continous indexes of those movies
    V_movies = V[movie_index] #trait vectors of movies rated by mth user
    b_movies = b_n[movie_index] #bias terms of movies rated by mth user
    b_m[m-1] = (np.sum(rate_vec - np.dot(V_movies,u_user) - b_movies))/len(movies) #bias term update
    sum1 = lamb_value*np.sum(V_movies[:, :, np.newaxis] * V_movies[:, np.newaxis, :], axis=0) + tau*np.identity(20)
    sum2 = lamb_value*np.sum(V_movies * (rate_vec - b_movies - np.full(len(movies), b_m[m-1])).reshape(len(movies), 1), axis = 0)
    U[m-1] = np.dot(np.linalg.inv(sum1), sum2) #update trait vector

#function for updating movie bias terms + trait vectors
def movies_update(n):
    global start_ind_mov; global Index_movie; global map_userId
    global U; global V; global b_n; global b_m; global m_usrs; global movie_map
    
    j = movie_map[n]  #index of nth movie
    rate_vec = np.array([t[1] for t in [Index_movie[i] for i in start_end_m(j)]]) #ratings given to nth movie
    v_movie = V[j] #trait vector of nth movie
    users = list([t[-1] for t in [map_userId[k] for k in start_end_m(j)]]) #users who rated jth movie
    U_users = U[[x-1 for x in users]] #trait vectors of users who rated nth movie
    b_users = b_m[[k-1 for k in users]] #bias terms of users who rated nth movie
    b_n[j] = (np.sum(rate_vec - np.dot(U_users,v_movie) - b_users))/len(users) #bias term update
    sum1 = lamb_value*np.sum(U_users[:, :, np.newaxis] * U_users[:, np.newaxis, :], axis=0) + tau*np.identity(20)
    sum2 = lamb_value*np.sum(U_users * (rate_vec - b_users - np.full(len(users), b_n[j])).reshape(len(users), 1), axis = 0)
    V[j] = np.dot(np.linalg.inv(sum1), sum2) #update trait vector
    



#function for obtaining cost function
def cost_update(q):
    global ratings_mov; global Index_movie; global start_ind_mov
    global n_movies; global U; global b_m; global b_n; global V
    
    j = movie_map[q] #index of qth movie
    rate_vec = np.array([t[1] for t in [Index_movie[l] for l in start_end_m(j)]]) #ratings array
    users = list([t[-1] for t in [map_userId[m] for m in start_end_m(j)]]) #users who rated jth movie
    U_users = U[[x - 1 for x in users]] #user traits of usrs who watched the qth movie
    b_users = b_m[[x - 1 for x in users]] #bias terms of users who watched qth movie
    b_movie = np.full(len(users), b_n[j])#bias of qth movie
    v_movie = V[j]  #trait vector of qth movie
    cost = np.sum(np.square(rate_vec - np.dot(U_users,v_movie) - b_users - b_movie))
    return cost

threshold = 10
cost = []
rmse_list = []
epoch = 0
while threshold > 0.1:
    #updating user biases
    for m in m_usrs:
        user_update(m) 
        
    #updating movie biases
    for n in n_movies:
        movies_update(n)
    
    #obtaining the cost function and rmse
    sum_cost = 0
    for q in n_movies:
        sum_cost += cost_update(q)
        
            
    cost.append(-(lamb_value/2)*sum_cost - (tau/2)*np.sum( np.sum(U*U, axis=1)) - (tau/2)*np.sum( np.sum(V*V, axis=1)))  
    rmse_list.append(np.sqrt((1/len(n_movies))*sum_cost))
    
    #monitor convergence
    if len(cost)>1:
        threshold = abs(cost[-1]-cost[-2])
        
    epoch += 1
    
    print("End of iteration: ", epoch)

# Cost function plot
x = list(range(len(cost)))
plt.plot(x, cost)
plt.xlabel("iteration")
plt.ylabel("Log Likelihood")
plt.title("Plot of Log Likelihood function")
plt.show()
print(cost[-1])

# RMSE plot
y = list(range(len(rmse_list)))
plt.plot(y, rmse_list)
plt.xlabel("iteration")
plt.ylabel("rmse")
plt.title("Plot of RMSE")
plt.show()
print(rmse_list[-1])

# dummy user predictions
def top_ratings(m):
    global U, V, b_n, movie_map, movies, ratings
    
    # obtain predicted ratings
    predicted_ratings = V.dot(U[m-1,:]) + 0.05*b_n
    
    # obtain the top predicted movie Id's and their names
    sorted_indices = np.argsort(-predicted_ratings)
    top_five_indices = sorted_indices[:5] #indexes of top 5 rated movies
    top_five_ratings = predicted_ratings[top_five_indices] #top five ratings
    top_movies = [movies.iloc[t,1] for t in top_five_indices] #movies names of this top rated movies
    top_genres = [movies.iloc[q,2] for q in top_five_indices] # corresponding genres
    movie_ratings = dict(zip(list(top_movies), list(top_genres))) #movies and their predicted ratings
    
    # movies movies that the user top rated
    user_m_ratings = ratings[ratings['userId']==m]
    # select top 5 movies that the user rated, and the ratings they gave
    top_rated = user_m_ratings.sort_values(by='rating', ascending=False)
    top_user_m = top_rated.head(5)

    #obtain list of top five movies, and that of the ratings
    best_rated = top_user_m['rating'].tolist() #list of top ratings
    movie_ids_top = top_user_m['movieId'].tolist() #movie ids of the top rated movies
    movie_names = [movies.iloc[movie_map[j],1] for j in movie_ids_top] #movies names of the top rated movies
    movie_genres = [movies.iloc[movie_map[w],2] for w in movie_ids_top] #obtain the genres of the top movies
    actual_top_rated = dict(zip(list(movie_names), list(movie_genres)))
    
    #print the top rated movies by user m
    print("Top 5 rated movies by this user: ", actual_top_rated)
    print("Top 5 predicted movies for this user")
    
    return movie_ratings

top_ratings(593)

### embeddings

import numpy as np
import pandas as pd
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from itertools import cycle

# Choose 20 random movies to include in the PCA analysis
np.random.seed(123)  # set random seed for reproducibility
movie_indices = np.random.choice(len(movies), size=20, replace=False)
features = V[movie_indices, :]

# Get the names and genres of the selected movies
movie_names = [movies.iloc[j,1] for j in movie_indices]
movie_genres = [movies.iloc[j,2] for j in movie_indices]

# Perform PCA to reduce dimensionality to 2
pca = PCA(n_components=2)
embeddings = pca.fit_transform(features)

# Assign a color to each movie point
colors = cycle('bgrcmyk')
color_dict = {}
for i, movie_name in enumerate(movie_names):
    color_dict[movie_name] = next(colors)

# Plot embeddings with movie names and genres as labels and legend
fig, ax = plt.subplots()

for i, movie_name in enumerate(movie_names):
    indices = np.where(np.array(movie_names) == movie_name)
    ax.scatter(embeddings[indices, 0], embeddings[indices, 1], label=movie_name + ' (' + movie_genres[i] + ')', color=color_dict[movie_name])
ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))
plt.show()

# embeddings

# Choose 20 random movies to include in the PCA analysis
np.random.seed(123)  # set random seed for reproducibility
movie_indices = np.random.choice(len(movies), size=20, replace=False)
features = V[movie_indices, :]

# Get the names and genres of the selected movies
movie_names = [movies.iloc[j,1] for j in movie_indices]
movie_genres = [movies.iloc[j,2] for j in movie_indices]

# Define a list of distinguishable colors
colors = ['#e6194B', '#3cb44b', '#ffe119', '#4363d8', '#f58231',          '#911eb4', '#42d4f4', '#f032e6', '#bfef45', '#fabed4',          '#469990', '#dcbeff', '#9A6324', '#fffac8', '#800000',          '#aaffc3', '#a9a9a9', '#000075', '#808000', '#ffd8b1']

# Assign a color to each movie point
color_dict = {}
for i, movie_name in enumerate(movie_names):
    color_dict[movie_name] = colors[i % len(colors)]

# Perform PCA to reduce dimensionality to 2
pca = PCA(n_components=2)
embeddings = pca.fit_transform(features)

# Plot embeddings with movie names and genres as labels and legend
fig, ax = plt.subplots()
for i, movie_name in enumerate(movie_names):
    indices = np.where(np.array(movie_names) == movie_name)
    ax.scatter(embeddings[indices, 0], embeddings[indices, 1], label=movie_name + ' (' + movie_genres[i] + ')', color=color_dict[movie_name])
ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))
plt.show()

## Start of Practical 2
# create a feature matrix
# split the genres column into a list of genres
movies['genres'] = movies['genres'].str.split('|')

# get a unique list of genres
genres = list(set([genre for movie_genres in movies['genres'] for genre in movie_genres]))

# create a dictionary to store the one-hot encoded genres
one_hot_dict = {}

# loop through each genre and encode it
for genre in genres:
    one_hot_dict[genre] = []
    for movie_genres in movies['genres']:
        if genre in movie_genres:
            one_hot_dict[genre].append(1)
        else:
            one_hot_dict[genre].append(0)

# create a new DataFrame from the one-hot encoded genres
# Horror, documentary, war, fantasy, romance, mystery, drama, thriller, western, film-noir,comedy,crime,imax,children, sci-fi, action,no-gen,animation,musical,adventure
one_hot_df = pd.DataFrame(one_hot_dict)
Features = one_hot_df.values

# concatenate the one-hot DataFrame with the original DataFrame
#df = pd.concat([df, one_hot_df], axis=1)

### Practical 2 - SGD for updates
# initialize biases
b_m_SGD = np.array([0]*len(m_usrs))   #initial bias terms of users
b_n_SGD = np.array([0]*len(n_movies)) #initial bias terms of movies

# initialize user and movie vectors
mean = 0
n_dimension = 20
std_dev = 2.5 / np.sqrt(10)
U_SGD = np.random.normal(loc=mean, scale=std_dev, size=(len(m_usrs), n_dimension))  #initial users trait vectors
V_SGD = np.random.normal(loc=mean, scale=std_dev, size=(len(n_movies), n_dimension)) #initial movies trait vectors

## Practical 2 - SGD for updates
learning_rate = 0.1
## update functions
def user_update_SGD(m):
    global start_ind_usr; global Index_user; global map_movieId
    global U_SGD; global V_SGD; global b_n_SGD; global b_m_SGD; global n_movies; global movie_map
    
    rate_vec = np.array([t[1] for t in [Index_user[i] for i in start_end_u(m)]]) #ratings given by mth user
    u_user = U_SGD[m-1] #trait vector of mth user
    b_user = b_m_SGD[m-1]       # bias term of the mth user
    movies =  list([t[-1] for t in [map_movieId[k] for k in start_end_u(m)]]) # movies rated by mth user
    random_index = random.randint(0, len(movies) - 1)            #randomly sample an index
    id_movie = movies[random_index]   #obtain the corresponsing random movie
    movie_index  = movie_map[id_movie]   #continous indexe of that movie
    V_movie = V_SGD[movie_index ]      #trait vectors of sampled movie
    b_movie = b_n_SGD[movie_index ]    #bias terms of sampled movie
    rate_movie = rate_vec[random_index]    #rating of that movie
    dL_db = lamb_value*(rate_movie - np.dot(u_user,V_movie) - b_movie) - lamb_value*len(rate_vec)*b_user #gradient of L wrt bias
    b_m_SGD[m-1] = b_user + learning_rate*dL_db #bias term update
    dL_du = lamb_value*(V_movie*(rate_movie - b_user - b_movie)) - lamb_value*np.dot(V_movie,np.dot(V_movie,u_user)) - tau*u_user #gradient of L wrt user trait vector
    U_SGD[m-1] = u_user + learning_rate*dL_du #user trait vector update
    
#function for updating movie bias terms + trait vectors
def movies_update_SGD(n):
    global start_ind_mov; global Index_movie; global map_userId
    global U_SGD; global V_SGD; global b_n_SGD; global b_m_SGD; global m_usrs; global movie_map
    
    j = movie_map[n]  #index of nth movie
    rate_vec = np.array([t[1] for t in [Index_movie[i] for i in start_end_m(j)]]) #ratings given to nth movie
    v_movie = V_SGD[j] #trait vector of nth movie
    b_movie = b_n_SGD[j]#bias term of nth movie
    users = list([t[-1] for t in [map_userId[k] for k in start_end_m(j)]]) #users who rated jth movie
    random_index = random.randint(0, len(users) - 1)  #randomly sample an index
    id_user = users[random_index]   #obtain the corresponsing random user
    U_user = U_SGD[id_user-1] #trait vector the user
    b_user = b_m_SGD[id_user-1] #bias term of the user
    rate_user = rate_vec[random_index]#rating of the movie by the user
    dL_db = lamb_value*(rate_user - np.dot(v_movie,U_user) - b_user) - lamb_value*len(rate_vec)*b_movie #gradient of L wrt bias
    b_n_SGD[j] = b_movie + learning_rate*dL_db #bias term update
    dL_dv = lamb_value*(U_user*(rate_user - b_user - b_movie)) - lamb_value*np.dot(U_user,np.dot(U_user,v_movie)) - tau*v_movie #gradient of L wrt user trait vector
    V_SGD[j] = v_movie + learning_rate*dL_dv #user trait vector update
    
    
#function for obtaining cost function
def cost_update_SGD(q):
    global ratings_mov; global Index_movie; global start_ind_mov
    global n_movies; global U_SGD; global b_m_SGD; global b_n_SGD; global V_SGD
    
    j = movie_map[q] #index of qth movie
    rate_vec = np.array([t[1] for t in [Index_movie[l] for l in start_end_m(j)]]) #ratings array
    users = list([t[-1] for t in [map_userId[m] for m in start_end_m(j)]]) #users who rated jth movie
    U_users = U_SGD[[x - 1 for x in users]] #user traits of usrs who watched the qth movie
    b_users = b_m_SGD[[x - 1 for x in users]] #bias terms of users who watched qth movie
    b_movie = np.full(len(users), b_n[j])#bias of qth movie
    v_movie = V_SGD[j]  #trait vector of qth movie
    cost = np.sum(np.square(rate_vec - np.dot(U_users,v_movie) - b_users - b_movie))
    return cost

#### Practical 2 - SGD
threshold = 10
epoch = 0
cost_SGD = []
rmse_list_SGD = []
while threshold > 0.1:
    #updating user biases
    for m in m_usrs:
        user_update_SGD(m) 
        
    #updating movie biases
    for n in n_movies:
        movies_update_SGD(n)
    
    #obtaining the cost function and rmse
    sum_cost = 0
    for q in n_movies:
        sum_cost += cost_update_SGD(q)
        
            
    cost_SGD.append(-(lamb_value/2)*sum_cost - (tau/2)*np.sum( np.sum(U_SGD*U_SGD, axis=1)) - (tau/2)*np.sum( np.sum(V_SGD*V_SGD, axis=1)))  
    rmse_list_SGD.append(np.sqrt((1/len(n_movies))*sum_cost))
    
    # monitor convergence
    if len(cost_SGD)>1:
        threshold = abs(cost_SGD[-1] - cost_SGD[-2])
        
    epoch += 1
    
    print("End of iteration: ", epoch)

# Cost function plot
x1 = list(range(len(cost_SGD)))
plt.plot(x1, cost_SGD)
plt.xlabel("iteration")
plt.ylabel("Log Likelihood")
plt.title("Plot of Log Likelihood function")
plt.show()

# RMSE plot
y1 = list(range(len(rmse_list_SGD)))
plt.plot(y1, rmse_list_SGD)
plt.xlabel("iteration")
plt.ylabel("rmse")
plt.title("Plot of RMSE")
plt.show()

# Recommendations using SGD
def top_ratings_SGD(m):
    global U_SGD, V_SGD, movie_map, movies, ratings
    
    # obtain predicted ratings
    predicted_ratings = V_SGD.dot(U_SGD[m-1,:]) 
    
    # obtain the top predicted movie Id's and their names
    sorted_indices = np.argsort(-predicted_ratings)
    top_five_indices = sorted_indices[:5] #indexes of top 5 rated movies
    top_five_ratings = predicted_ratings[top_five_indices] #top five ratings
    top_movies = [movies.iloc[t,1] for t in top_five_indices] #movies names of this top rated movies
    top_genres = [movies.iloc[q,2] for q in top_five_indices] # corresponding genres
    movie_ratings = dict(zip(list(top_movies), list(top_genres))) #movies and their predicted ratings
    
    # movies movies that the user top rated
    user_m_ratings = ratings[ratings['userId']==m]
    # select top 5 movies that the user rated, and the ratings they gave
    top_rated = user_m_ratings.sort_values(by='rating', ascending=False)
    top_user_m = top_rated.head(5)

    #obtain list of top five movies, and that of the ratings
    best_rated = top_user_m['rating'].tolist() #list of top ratings
    movie_ids_top = top_user_m['movieId'].tolist() #movie ids of the top rated movies
    movie_names = [movies.iloc[movie_map[j],1] for j in movie_ids_top] #movies names of the top rated movies
    movie_genres = [movies.iloc[movie_map[w],2] for w in movie_ids_top] #obtain the genres of the top movies
    actual_top_rated = dict(zip(list(movie_names), list(movie_genres)))
    
    #print the top rated movies by user m
    print("Top 5 rated movies by this user: ", actual_top_rated)
    print("Top 5 predicted movies for this user")
    
    return movie_ratings

top_ratings_SGD(593)

### SGD Embeddings of feature vectors
# Choose 20 random movies to include in the PCA analysis
np.random.seed(123)  # set random seed for reproducibility
movie_indices = np.random.choice(len(movies), size=20, replace=False)
features = V_SGD[movie_indices, :]

# Get the names and genres of the selected movies
movie_names = [movies.iloc[j,1] for j in movie_indices]
movie_genres = [movies.iloc[j,2] for j in movie_indices]

# Define a list of distinguishable colors
colors = ['#e6194B', '#3cb44b', '#ffe119', '#4363d8', '#f58231',        '#911eb4', '#42d4f4', '#f032e6', '#bfef45', '#fabed4',          '#469990', '#dcbeff', '#9A6324', '#fffac8', '#800000',          '#aaffc3', '#a9a9a9', '#000075', '#808000', '#ffd8b1']

# Assign a color to each movie point
color_dict = {}
for i, movie_name in enumerate(movie_names):
    color_dict[movie_name] = colors[i % len(colors)]

# Perform PCA to reduce dimensionality to 2
pca = PCA(n_components=2)
embeddings = pca.fit_transform(features)

# Plot embeddings with movie names and genres as labels and legend
fig, ax = plt.subplots()
for i, movie_name in enumerate(movie_names):
    indices = np.where(np.array(movie_names) == movie_name)
    ax.scatter(embeddings[indices, 0], embeddings[indices, 1], label=movie_name + ' (' + str(movie_genres[i]) + ')', color=color_dict[movie_name])
ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))
plt.show()

rmse_list_SGD[-1]

### Practical 2 - Bayesian Personalized Ranking (BPR)
import math
# initialize the trait vectors
alpha_rate = 0.1
lambda_rate = 0.01
mean = 0
n_dimension = 20
std_dev = 2.5 / np.sqrt(10)
W = np.random.normal(loc=mean, scale=std_dev, size=(len(m_usrs), n_dimension))  #initial users trait vectors
H = np.random.normal(loc=mean, scale=std_dev, size=(len(n_movies), n_dimension)) #initial movies trait vectors

#logistic sigmoid function
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

#function for obtaining cost function
def lik_update(q):
    global ratings_mov, Index_movie, start_ind_mov, n_movies, W, H
    
    likelihood = 0
    movies_q = [t[-1] for t in [map_movieId[k] for k in start_end_u(q)]] # movies rated by qth user
    cont_index = [movie_map[j] for j in movies_q]   #continuous indexes of those movies watched by qth user
    not_watched = [t for t in movie_Id if t not in cont_index]     #indexes of movies not watched by the qth user
    q_vec = W[q-1] #trait vector of qth user
    
    # Compute the BPR-OPT
    H_watched = H[cont_index, :]  #subset of H for watched movies
    H_not_watched = H[not_watched, :]  #subset of H for non-watched movies
    x_uij = np.dot(H_watched, q_vec)[:, None] - np.dot(H_not_watched, q_vec)[:, None].T
    likelihood = np.sum(np.log(sigmoid(x_uij)))
    
    return likelihood

### Practical 2 - Bayesian Personalized Ranking (BPR)
BPR_OPT = []
convergence = 10
iter_count = 0
#convergence>0.0001
while iter_count<201:
    # draw random (user, movie1, movie2)
    u = random.choice(m_usrs)  # sample a random user
    movies_u =  list([t[-1] for t in [map_movieId[k] for k in start_end_u(u)]]) # movies rated by uth user
    cont_index  = list([movie_map[j] for j in movies_u])   #continous indexes of those movies
    i = random.choice(cont_index)   #sample one random movie watched by this user
    j = random.choice([q for q in movie_Id if q != i]) #sample one random movie not watched by this user
    
    # calculate special difference between user u, movie i, and movie j
    x_uij = np.dot(W[u-1],H[i]) - np.dot(W[u-1],H[j])
    
    # update these selected parameters
    W[u-1] = W[u-1] + alpha_rate*((math.exp(x_uij)/(1 + math.exp(x_uij)))*(H[i] - H[j]) + lambda_rate*W[u-1])
    H[i] = H[i] + alpha_rate*((math.exp(x_uij)/(1 + math.exp(x_uij)))*W[u-1] + lambda_rate*H[i])
    H[j] = H[j] + alpha_rate*((math.exp(x_uij)/(1 + math.exp(x_uij)))*(-1*W[u-1]) + lambda_rate*H[j])
    
    #update the cost and monitor convergence
    #obtaining the cost function and rmse
    sum_BPR = 0
    for q in m_usrs:
        sum_BPR += lik_update(q)
        
    BPR_OPT.append(sum_BPR - lambda_rate*(np.linalg.norm(W, ord='fro')**2 + np.linalg.norm(H, ord='fro')**2 ) )       
    
    
    # monitor convergence
    if len(BPR_OPT)>1:
        convergence = abs(BPR_OPT[-1] - BPR_OPT[-2])
        
    iter_count += 1
    
    print("End of iteration ", iter_count)
        
        
# Cost function plot
x2 = list(range(len(BPR_OPT)))
plt.plot(x2, BPR_OPT)
plt.xlabel("iteration")
plt.ylabel("BPR-OPT")
plt.title("Plot of BPR-OPT")
plt.show()

### BPR Embeddings of feature vectors
# Choose 20 random movies to include in the PCA analysis
np.random.seed(123)  # set random seed for reproducibility
movie_indices = np.random.choice(len(movies), size=20, replace=False)
features = H[movie_indices, :]

# Get the names and genres of the selected movies
movie_names = [movies.iloc[j,1] for j in movie_indices]
movie_genres = [movies.iloc[j,2] for j in movie_indices]

# Define a list of distinguishable colors
colors = ['#e6194B', '#3cb44b', '#ffe119', '#4363d8', '#f58231',          '#911eb4', '#42d4f4', '#f032e6', '#bfef45', '#fabed4',          '#469990', '#dcbeff', '#9A6324', '#fffac8', '#800000',          '#aaffc3', '#a9a9a9', '#000075', '#808000', '#ffd8b1']

# Assign a color to each movie point
color_dict = {}
for i, movie_name in enumerate(movie_names):
    color_dict[movie_name] = colors[i % len(colors)]

# Perform PCA to reduce dimensionality to 2
pca = PCA(n_components=2)
embeddings = pca.fit_transform(features)

# Plot embeddings with movie names and genres as labels and legend
fig, ax = plt.subplots()
for i, movie_name in enumerate(movie_names):
    indices = np.where(np.array(movie_names) == movie_name)
    ax.scatter(embeddings[indices, 0], embeddings[indices, 1], label=movie_name + ' (' + movie_genres[i] + ')', color=color_dict[movie_name])
ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))
plt.show()

# dummy user predictions
def top_ratings_BPR(m):
    global W, H, movie_map, movies, ratings
    
    # obtain predicted ratings
    predicted_ratings = H.dot(W[m-1,:]) 
    
    # obtain the top predicted movie Id's and their names
    sorted_indices = np.argsort(-predicted_ratings)
    top_five_indices = sorted_indices[:5] #indexes of top 5 rated movies
    top_five_ratings = predicted_ratings[top_five_indices] #top five ratings
    top_movies = [movies.iloc[t,1] for t in top_five_indices] #movies names of this top rated movies
    top_genres = [movies.iloc[q,2] for q in top_five_indices] # corresponding genres
    movie_ratings = dict(zip(list(top_movies), list(top_genres))) #movies and their predicted ratings
    
    # movies movies that the user top rated
    user_m_ratings = ratings[ratings['userId']==m]
    # select top 5 movies that the user rated, and the ratings they gave
    top_rated = user_m_ratings.sort_values(by='rating', ascending=False)
    top_user_m = top_rated.head(5)

    #obtain list of top five movies, and that of the ratings
    best_rated = top_user_m['rating'].tolist() #list of top ratings
    movie_ids_top = top_user_m['movieId'].tolist() #movie ids of the top rated movies
    movie_names = [movies.iloc[movie_map[j],1] for j in movie_ids_top] #movies names of the top rated movies
    movie_genres = [movies.iloc[movie_map[w],2] for w in movie_ids_top] #obtain the genres of the top movies
    actual_top_rated = dict(zip(list(movie_names), list(movie_genres)))
    
    #print the top rated movies by user m
    print("Top 5 rated movies by this user: ", actual_top_rated)
    print("Top 5 predicted movies for this user")
    
    return movie_ratings

top_ratings_BPR(11)